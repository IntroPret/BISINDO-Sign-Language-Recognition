{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Install necessary packages ---\n",
        "!pip install ultralytics gdown pandas -q\n",
        "\n",
        "# --- Import libraries ---\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from ultralytics import YOLO\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications import VGG19, MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"✅ Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-qvexgR4Dk3",
        "outputId": "f0223bbb-a1da-4c36-ca51-4a1f44df36cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Main dataset directory in your Google Drive\n",
        "DATA_DIR = '/content/drive/MyDrive/bisindo_dataset_split'\n",
        "train_dir = os.path.join(DATA_DIR, 'train')\n",
        "val_dir = os.path.join(DATA_DIR, 'val')\n",
        "test_dir = os.path.join(DATA_DIR, 'test')\n",
        "\n",
        "# Model and Image Parameters\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 26 # Based on your training notebooks\n",
        "NUM_TEST_IMAGES = 160\n",
        "EPOCHS = 15 # Set a reasonable number of epochs for training\n",
        "\n",
        "# To store final results\n",
        "comparison_results = []"
      ],
      "metadata": {
        "id": "lznTj9m04DnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Datasets for TensorFlow ---\n",
        "print(\"Loading TensorFlow datasets...\")\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# --- 1. TRAIN AND BENCHMARK MOBILENETV2 (Correct Architecture) ---\n",
        "print(\"\\n======================================\")\n",
        "print(\"     Training MobileNetV2\")\n",
        "print(\"======================================\")\n",
        "\n",
        "# Build Model\n",
        "base_mobilenet = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
        "base_mobilenet.trainable = False\n",
        "\n",
        "# CORRECTED: This now matches your original MobileNetV2 notebook\n",
        "mobilenet_model = Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    base_mobilenet,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.5), # Correct Dropout for MobileNetV2\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "mobilenet_model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "mobilenet_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[early_stopping])\n",
        "mobilenet_model.save('mobilenetv2_best.h5')\n",
        "print(\"✅ MobileNetV2 model trained and saved.\")\n",
        "\n",
        "# Benchmark Model\n",
        "print(\"\\n--- Benchmarking MobileNetV2 ---\")\n",
        "test_ds_inf = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir, seed=123, image_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "inference_samples = test_ds_inf.take(NUM_TEST_IMAGES // BATCH_SIZE)\n",
        "# GPU Warm-up\n",
        "for images, _ in inference_samples.take(1): _ = mobilenet_model.predict(images, verbose=0)\n",
        "# Timed Inference\n",
        "total_time, num_images = 0, 0\n",
        "for images, _ in inference_samples:\n",
        "    start_time = time.time()\n",
        "    _ = mobilenet_model.predict(images, verbose=0)\n",
        "    end_time = time.time()\n",
        "    total_time += (end_time - start_time)\n",
        "    num_images += len(images)\n",
        "avg_time_ms = (total_time / num_images) * 1000\n",
        "comparison_results.append({\n",
        "    'Model': 'MobileNetV2',\n",
        "    'Parameters': f\"{mobilenet_model.count_params():,}\",\n",
        "    'Inference Time (ms/image)': f\"{avg_time_ms:.4f}\"\n",
        "})\n",
        "print(f\"✅ MobileNetV2 Benchmark Done: {avg_time_ms:.4f} ms/image\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyP5lurI4Dpv",
        "outputId": "64f8da9a-1c83-4673-c37d-2a0eda706c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TensorFlow datasets...\n",
            "Found 3126 files belonging to 26 classes.\n",
            "Using 2501 files for training.\n",
            "Found 670 files belonging to 26 classes.\n",
            "Using 134 files for validation.\n",
            "\n",
            "======================================\n",
            "     Training MobileNetV2\n",
            "======================================\n",
            "Epoch 1/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m942s\u001b[0m 12s/step - accuracy: 0.0559 - loss: 4.7119 - val_accuracy: 0.1269 - val_loss: 3.7085\n",
            "Epoch 2/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 98ms/step - accuracy: 0.1764 - loss: 3.7299 - val_accuracy: 0.2836 - val_loss: 3.2636\n",
            "Epoch 3/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 108ms/step - accuracy: 0.3113 - loss: 3.1999 - val_accuracy: 0.3507 - val_loss: 2.9693\n",
            "Epoch 4/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 99ms/step - accuracy: 0.4007 - loss: 2.7862 - val_accuracy: 0.4104 - val_loss: 2.7627\n",
            "Epoch 5/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 100ms/step - accuracy: 0.4444 - loss: 2.6186 - val_accuracy: 0.4776 - val_loss: 2.6197\n",
            "Epoch 6/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.5063 - loss: 2.3864 - val_accuracy: 0.4851 - val_loss: 2.5293\n",
            "Epoch 7/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.5355 - loss: 2.2614 - val_accuracy: 0.5224 - val_loss: 2.4436\n",
            "Epoch 8/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.5991 - loss: 2.0623 - val_accuracy: 0.5224 - val_loss: 2.3806\n",
            "Epoch 9/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.6235 - loss: 1.9782 - val_accuracy: 0.5224 - val_loss: 2.3454\n",
            "Epoch 10/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.6728 - loss: 1.8561 - val_accuracy: 0.5299 - val_loss: 2.2852\n",
            "Epoch 11/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 108ms/step - accuracy: 0.6794 - loss: 1.7986 - val_accuracy: 0.5522 - val_loss: 2.2373\n",
            "Epoch 12/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.7169 - loss: 1.7075 - val_accuracy: 0.5672 - val_loss: 2.2139\n",
            "Epoch 13/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 95ms/step - accuracy: 0.7455 - loss: 1.6151 - val_accuracy: 0.5970 - val_loss: 2.1937\n",
            "Epoch 14/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.7774 - loss: 1.5380 - val_accuracy: 0.5970 - val_loss: 2.1548\n",
            "Epoch 15/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 98ms/step - accuracy: 0.7877 - loss: 1.5035 - val_accuracy: 0.5970 - val_loss: 2.1332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ MobileNetV2 model trained and saved.\n",
            "\n",
            "--- Benchmarking MobileNetV2 ---\n",
            "Found 670 files belonging to 26 classes.\n",
            "✅ MobileNetV2 Benchmark Done: 3.0759 ms/image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. TRAIN AND BENCHMARK VGG19 (Correct Architecture) ---\n",
        "print(\"\\n======================================\")\n",
        "print(\"         Training VGG19\")\n",
        "print(\"======================================\")\n",
        "\n",
        "# Build Model\n",
        "base_vgg19 = VGG19(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
        "base_vgg19.trainable = False\n",
        "\n",
        "# CORRECTED: This now matches your original VGG19 notebook\n",
        "vgg19_model = Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    base_vgg19,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.4), # Correct Dropout for VGG19\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "vgg19_model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "vgg19_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[early_stopping])\n",
        "vgg19_model.save('vgg19_best.h5')\n",
        "print(\"✅ VGG19 model trained and saved.\")\n",
        "\n",
        "# Benchmark Model\n",
        "print(\"\\n--- Benchmarking VGG19 ---\")\n",
        "# GPU Warm-up\n",
        "for images, _ in inference_samples.take(1): _ = vgg19_model.predict(images, verbose=0)\n",
        "# Timed Inference\n",
        "total_time, num_images = 0, 0\n",
        "for images, _ in inference_samples:\n",
        "    start_time = time.time()\n",
        "    _ = vgg19_model.predict(images, verbose=0)\n",
        "    end_time = time.time()\n",
        "    total_time += (end_time - start_time)\n",
        "    num_images += len(images)\n",
        "avg_time_ms = (total_time / num_images) * 1000\n",
        "comparison_results.append({\n",
        "    'Model': 'VGG19',\n",
        "    'Parameters': f\"{vgg19_model.count_params():,}\",\n",
        "    'Inference Time (ms/image)': f\"{avg_time_ms:.4f}\"\n",
        "})\n",
        "print(f\"✅ VGG19 Benchmark Done: {avg_time_ms:.4f} ms/image\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLkyWI9rLCuD",
        "outputId": "e06c77fa-36a8-4851-bd96-106efce0a52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================\n",
            "         Training VGG19\n",
            "======================================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 359ms/step - accuracy: 0.0555 - loss: 4.2118 - val_accuracy: 0.0224 - val_loss: 3.7974\n",
            "Epoch 2/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 223ms/step - accuracy: 0.1257 - loss: 3.6610 - val_accuracy: 0.0448 - val_loss: 3.6939\n",
            "Epoch 3/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 226ms/step - accuracy: 0.2112 - loss: 3.3419 - val_accuracy: 0.1269 - val_loss: 3.5618\n",
            "Epoch 4/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 233ms/step - accuracy: 0.2630 - loss: 3.0763 - val_accuracy: 0.2090 - val_loss: 3.3788\n",
            "Epoch 5/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 231ms/step - accuracy: 0.3292 - loss: 2.8842 - val_accuracy: 0.3284 - val_loss: 3.1613\n",
            "Epoch 6/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 228ms/step - accuracy: 0.3663 - loss: 2.6971 - val_accuracy: 0.3731 - val_loss: 2.9430\n",
            "Epoch 7/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 227ms/step - accuracy: 0.4010 - loss: 2.5652 - val_accuracy: 0.3657 - val_loss: 2.7691\n",
            "Epoch 8/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 229ms/step - accuracy: 0.4573 - loss: 2.4086 - val_accuracy: 0.3806 - val_loss: 2.6359\n",
            "Epoch 9/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 229ms/step - accuracy: 0.4806 - loss: 2.3052 - val_accuracy: 0.3955 - val_loss: 2.5410\n",
            "Epoch 10/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 229ms/step - accuracy: 0.5088 - loss: 2.1794 - val_accuracy: 0.4104 - val_loss: 2.4632\n",
            "Epoch 11/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 229ms/step - accuracy: 0.5472 - loss: 2.1083 - val_accuracy: 0.4627 - val_loss: 2.4108\n",
            "Epoch 12/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 229ms/step - accuracy: 0.5451 - loss: 2.0504 - val_accuracy: 0.4552 - val_loss: 2.3265\n",
            "Epoch 13/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 230ms/step - accuracy: 0.5832 - loss: 1.9756 - val_accuracy: 0.4552 - val_loss: 2.2842\n",
            "Epoch 14/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 228ms/step - accuracy: 0.5904 - loss: 1.8997 - val_accuracy: 0.4776 - val_loss: 2.2202\n",
            "Epoch 15/15\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 228ms/step - accuracy: 0.6130 - loss: 1.8300 - val_accuracy: 0.4925 - val_loss: 2.1875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ VGG19 model trained and saved.\n",
            "\n",
            "--- Benchmarking VGG19 ---\n",
            "✅ VGG19 Benchmark Done: 8.5736 ms/image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. DOWNLOAD AND BENCHMARK YOLOv8 ---\n",
        "print(\"\\n======================================\")\n",
        "print(\"        Benchmarking YOLOv8\")\n",
        "print(\"=======================================\")\n",
        "\n",
        "# --- Your YOLOv8 Model File Path from Google Drive ---\n",
        "# NOTE: Using a direct path is better since your Drive is already mounted.\n",
        "# Make sure this path is correct for your Google Drive setup.\n",
        "yolov8_model_path = '/content/drive/My Drive/Colab Notebooks/best.pt' # <--- VERIFY THIS PATH\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(yolov8_model_path):\n",
        "    raise FileNotFoundError(f\"YOLOv8 model not found at: {yolov8_model_path}. Please verify the path.\")\n",
        "\n",
        "# Load Model\n",
        "print(\"Loading YOLOv8 model...\")\n",
        "yolo_model = YOLO(yolov8_model_path)\n",
        "print(\"✅ YOLOv8 model loaded.\")\n",
        "\n",
        "\n",
        "# !! CORRECTON: Explicitly move the model to the GPU !!\n",
        "print(\"Moving model to GPU...\")\n",
        "yolo_model.to('cuda')\n",
        "print(\"✅ Model moved to GPU.\")\n",
        "\n",
        "# Standard params for yolov8n-cls\n",
        "yolo_params = 1468186\n",
        "\n",
        "# Prepare Image Paths\n",
        "test_image_paths = []\n",
        "for root, _, files in os.walk(test_dir):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            test_image_paths.append(os.path.join(root, file))\n",
        "sample_image_paths = test_image_paths[:NUM_TEST_IMAGES]\n",
        "\n",
        "# GPU Warm-up\n",
        "if sample_image_paths:\n",
        "    _ = yolo_model.predict(sample_image_paths[0], verbose=False)\n",
        "\n",
        "# Timed Inference on GPU\n",
        "print(\"Running timed inference on GPU...\")\n",
        "start_time = time.time()\n",
        "_ = yolo_model.predict(sample_image_paths, verbose=False)\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "avg_time_ms = (total_time / len(sample_image_paths)) * 1000\n",
        "\n",
        "comparison_results.append({\n",
        "    'Model': 'YOLOv8n-cls',\n",
        "    'Parameters': f\"{yolo_params:,}\",\n",
        "    'Inference Time (ms/image)': f\"{avg_time_ms:.4f}\"\n",
        "})\n",
        "print(f\"✅ YOLOv8 Benchmark Done: {avg_time_ms:.4f} ms/image\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FONYmFJb4Dr_",
        "outputId": "1368f755-3157-409f-cc63-c30aa41d043c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================\n",
            "        Benchmarking YOLOv8\n",
            "=======================================\n",
            "Loading YOLOv8 model...\n",
            "✅ YOLOv8 model loaded.\n",
            "Moving model to GPU...\n",
            "✅ Model moved to GPU.\n",
            "Running timed inference on GPU...\n",
            "✅ YOLOv8 Benchmark Done: 5.2247 ms/image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Display Results Table ---\n",
        "print(\"\\n======================================\")\n",
        "print(\"     Final Model Comparison\")\n",
        "print(\"======================================\")\n",
        "\n",
        "if comparison_results:\n",
        "    df = pd.DataFrame(comparison_results)\n",
        "    # Ensure consistent column order\n",
        "    df = df[['Model', 'Parameters', 'Inference Time (ms/image)']]\n",
        "    print(df.to_string(index=False))\n",
        "else:\n",
        "    print(\"No results were generated.\")\n",
        "\n",
        "print(\"======================================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbfvQQW24L1o",
        "outputId": "47ff7469-c37a-42c5-fb87-035df093f71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================\n",
            "     Final Model Comparison\n",
            "======================================\n",
            "      Model Parameters Inference Time (ms/image)\n",
            "MobileNetV2  2,929,242                    3.0759\n",
            "      VGG19 20,302,426                    8.5736\n",
            "YOLOv8n-cls  1,468,186                  774.0325\n",
            "YOLOv8n-cls  1,468,186                    5.2247\n",
            "======================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ykaBsgUD4MAD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}